import argparse
import logging
import math
import os

import deepspeed
import torch
import torch.nn.functional as F
from deepspeed import get_accelerator
from diffusers import (AutoencoderKL, DDPMScheduler, StableDiffusionPipeline,
                       UNet2DConditionModel)
from diffusers.training_utils import EMAModel
from diffusers.utils import convert_state_dict_to_diffusers
from peft import LoraConfig, get_peft_model_state_dict
from tqdm.auto import tqdm
from transformers import CLIPTextModel, CLIPTokenizer
from utils import (deepspeed_config_from_args, is_rank_0, load_custom_dataset,
                   load_training_config, log_validation, set_random_seed)


def main():
    parser = argparse.ArgumentParser(description='deepspeedè®­ç»ƒSDè„šæœ¬')
    parser.add_argument('--local_rank', type=int, default=-1,
                        help='ä¼ é€’ç»™deepspeedçš„hook,é™¤äº†å¤šèŠ‚ç‚¹Slurmï¼Œå…¶å®ƒå¯åŠ¨ä¼šè‡ªåŠ¨è¡¥å……')
    parser.add_argument('--cfg', type=str, default="./default.json", help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    cfg_path = args.cfg
    cfg = load_training_config(cfg_path)

    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )

    if cfg.offline:
        os.environ["WANDB_MODE"] = "offline"
    # å…¼å®¹å•æœºï¼Œå•æœºå¤šå¡æ²¡æœ‰è¿™ä¸ªå˜é‡
    if os.environ.get('SLURM_NTASKS'):
        os.environ['RANK'] = os.environ['SLURM_PROCID']
        os.environ['WORLD_SIZE'] = os.environ['SLURM_NTASKS']
        os.environ['MASTER_PORT'] = os.environ['MASTER_PORT']
        os.environ['LOCAL_RANK'] = os.environ['SLURM_LOCALID']
        args.local_rank = int(os.environ['SLURM_LOCALID'])

    if args.local_rank == -1:
        device = torch.device(get_accelerator().device_name())
    else:
        get_accelerator().set_device(args.local_rank)
        device = torch.device(get_accelerator().device_name(), args.local_rank)
        deepspeed.init_distributed()

    args.global_rank = torch.distributed.get_rank()
    if "seed" in cfg:
        # è®¾ç½®éšæœºæ•°ï¼Œä¿è¯ç»“æžœå¯éªŒè¯
        set_random_seed(cfg.seed)

    logging.info("æ¨¡åž‹åˆå§‹åŒ–ä¸­ðŸ˜‰")
    # åˆå§‹åŒ–æ¨¡åž‹
    if cfg.use_fp16.enabled:
        weight_dtype = torch.float16
    else:
        weight_dtype = torch.float32

    # ä»Žé¢„è®­ç»ƒæ¨¡åž‹ä¸­åŠ è½½æ¨¡åž‹
    noise_scheduler = DDPMScheduler.from_pretrained(cfg.pretrained_model_name_or_path, subfolder="scheduler")
    tokenizer = CLIPTokenizer.from_pretrained(cfg.pretrained_model_name_or_path, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(cfg.pretrained_model_name_or_path, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(cfg.pretrained_model_name_or_path, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(cfg.pretrained_model_name_or_path, subfolder="unet")
    # å†»ç»“VAEå’Œtext_encoder
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)

    if cfg.use_lora.action:
        # å†»ç»“Unetçš„æƒé‡
        unet.requires_grad_(False)
        unet_lora_config = LoraConfig(
            r=cfg.use_lora.rank,
            lora_alpha=cfg.use_lora.alpha,
            init_lora_weights="gaussian",
            target_modules=["to_k", "to_q", "to_v", "to_out.0"],
        )
        cfg.checkpoint_dir = cfg.checkpoint_dir + "-lora"
        # ç§»åŠ¨åˆ°GPU
        unet.to(device, dtype=weight_dtype)
        text_encoder.to(device, dtype=weight_dtype)
        vae.to(device, dtype=weight_dtype)
        # å¢žåŠ é€‚é…å™¨
        unet.add_adapter(unet_lora_config)
        if weight_dtype == torch.float16:
            for param in unet.parameters():
                # è®­ç»ƒLoRAçš„å‚æ•°åªèƒ½æ˜¯fp32
                if param.requires_grad:
                    param.data = param.to(torch.float32)
    else:
        # ç§»åŠ¨åˆ°GPU
        text_encoder.to(device, dtype=weight_dtype)
        vae.to(device, dtype=weight_dtype)
        if cfg.use_ema:
            ema_unet = EMAModel(unet.parameters(), model_cls=UNet2DConditionModel, model_config=unet.config)
            ema_unet.to(device)

    logging.info("åˆå§‹åŒ–æ•°æ®é›†ðŸ³")
    # åˆå§‹åŒ–æ•°æ®é›†
    train_dataset, collate_fn = load_custom_dataset(cfg, tokenizer, imagefolder=cfg.imagefolder)
    if args.local_rank != -1:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None

    train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                                   collate_fn=collate_fn,
                                                   shuffle=(train_sampler is None),
                                                   batch_size=cfg.train_micro_batch_size_per_gpu,
                                                   sampler=train_sampler,
                                                   num_workers=cfg.dataloader_num_workers,
                                                   pin_memory=True)

    logging.info("DeepSpeedè£…è½½ingðŸ› ï¸")
    # åˆå§‹åŒ–å¼•æ“Ž
    deepspeed_config = deepspeed_config_from_args(cfg)
    if deepspeed_config["scheduler"]["type"].startswith("Warm"):
        deepspeed_config["scheduler"]["params"]["warmup_max_lr"] = deepspeed_config["optimizer"]["params"]["lr"]
        deepspeed_config["scheduler"]["params"]["warmup_num_steps"] = cfg.lr_warmup_steps * int(
            os.environ['WORLD_SIZE'])
    torch.distributed.barrier()
    parameters = filter(lambda p: p.requires_grad, unet.parameters())
    unet, optimizer, _, lr_scheduler = deepspeed.initialize(
        args=args,
        model=unet,
        model_parameters=parameters,
        config=deepspeed_config,
    )

    # Train!
    total_batch_size = cfg.train_micro_batch_size_per_gpu * int(
        os.environ['WORLD_SIZE']) * cfg.gradient_accumulation_steps
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / cfg.gradient_accumulation_steps)

    if cfg.max_train_steps == 0:
        cfg.max_train_steps = math.ceil(cfg.num_epochs * num_update_steps_per_epoch)
    else:
        cfg.num_epochs = math.ceil(cfg.max_train_steps / num_update_steps_per_epoch)

    logging.info("***** å¼€å§‹è®­ç»ƒå•¦ï¼ðŸš€*****")
    logging.info(f"  æ•°æ®æ ·æœ¬æ•°é‡ = {len(train_dataset)}")
    logging.info(f"  æ¯ä¸ªè®¾å¤‡çš„æ•°æ®æ ·æœ¬æ•°é‡ = {len(train_dataloader)}")
    logging.info(f"  è®­ç»ƒè½®æ•° = {cfg.num_epochs}")
    logging.info(f"  æ¯ä¸ªè®¾å¤‡çš„Batch size = {cfg.train_micro_batch_size_per_gpu}")
    logging.info(f"  æ€»çš„è®­ç»ƒæ‰¹å¤„ç†å¤§å°ï¼ˆåŒ…æ‹¬å¹¶è¡Œï¼Œåˆ†å¸ƒå¼å’Œç´¯ç§¯ï¼‰ = {total_batch_size}")
    logging.info(f"  æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ = {cfg.gradient_accumulation_steps}")
    logging.info(f"  æ€»ä¼˜åŒ–æ­¥éª¤ = {cfg.max_train_steps}")

    global_step = 0
    first_epoch = 0
    if cfg.resume_from_checkpoint:
        # åŠ è½½æ£€æŸ¥ç‚¹
        # ä½¿ç”¨loraè®­ç»ƒçš„æ—¶å€™ä¸è¦åŠ è½½åŽŸæ¥çš„æƒé‡äº†ï¼Œå› ä¸ºä¿å­˜çš„æƒé‡å¹¶ä¸æ˜¯Unetæƒé‡ï¼Œå¯¼å…¥ä¼šå‡ºé”™
        if os.path.exists(cfg.checkpoint_dir) and not cfg.use_lora.action:
            unet.load_checkpoint(f"./{cfg.checkpoint_dir}/")
            latest_file_path = os.path.join(f"./{cfg.checkpoint_dir}", "latest")
            with open(latest_file_path, "r") as file:
                content = file.read().strip()
                step_str = content.split('step')[-1]
                global_step = int(step_str)
                initial_global_step = global_step
                first_epoch = int(global_step // num_update_steps_per_epoch)
        else:
            logging.info("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†å¼€å§‹æ–°çš„è®­ç»ƒè¿‡ç¨‹ã€‚")
            cfg.resume_from_checkpoint = False
            initial_global_step = 0
    else:
        initial_global_step = 0

    progress_bar = tqdm(
        range(0, cfg.max_train_steps),
        initial=initial_global_step,
        desc="Steps",
        disable=not is_rank_0(),
    )

    for epoch in range(first_epoch, cfg.num_epochs):
        unet.train()
        for step, batch in enumerate(train_dataloader):
            if global_step >= cfg.max_train_steps:
                break
            with torch.no_grad():
                images, texts = batch['pixel_values'].to(unet.device, dtype=weight_dtype, non_blocking=True), batch[
                    'input_ids'].to(
                    unet.device, non_blocking=True)
                # å°†ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾åƒè½¬æ¢ä¸ºæ½œç©ºé—´è¡¨ç¤º
                latents = vae.encode(images).latent_dist.sample()
                latents = latents * vae.config.scaling_factor
                # ç”Ÿæˆé«˜æ–¯å™ªå£°
                noise = torch.randn_like(latents)
                # ä¸ºæ‰¹æ¬¡é‡Œçš„æ¯å¼ å›¾ç‰‡éšæœºé€‰æ‹©ä¸€ä¸ªæ—¶é—´æ­¥
                batch_size = latents.shape[0]
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,),
                                          device=latents.device)
                timesteps = timesteps.long()
                # å‰å‘è¿‡ç¨‹
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # å¤„ç†æ–‡æœ¬
                text_tokens = text_encoder(texts, return_dict=False)[0]

                # ç›®æ ‡å™ªå£°
                if noise_scheduler.config.prediction_type == "epsilon":
                    target = noise
                elif noise_scheduler.config.prediction_type == "v_prediction":
                    target = noise_scheduler.get_velocity(
                        latents, noise, timesteps)
                else:
                    raise ValueError(
                        f"Unknown prediction type {noise_scheduler.config.prediction_type}")

            # é¢„æµ‹å™ªå£°æ®‹å·®å¹¶è®¡ç®—æŸå¤±
            model_pred = unet(noisy_latents, timesteps, text_tokens, return_dict=False)[0]

            loss = F.mse_loss(model_pred.float(),
                              target.float(), reduction="mean")
            unet.backward(loss)
            unet.step()
            if not cfg.use_lora.action and cfg.use_ema:
                ema_unet.step(unet.module.parameters())
            if step % cfg.gradient_accumulation_steps == 0:
                logs = {"epoch": f"{epoch + 1 : d}", "loss": f"{loss.item():.6f}"}
                progress_bar.set_postfix(**logs)
                progress_bar.update(1)
                if step % cfg.save_interval == 0 and is_rank_0():
                    unet.save_checkpoint(f"{cfg.checkpoint_dir}")
                    logging.info("è½¬æ¢ä¸ºSDæƒé‡...")
                    if not cfg.use_lora.action:
                        if cfg.use_ema:
                            ema_unet.copy_to(unet.module.parameters())
                            ema_unet.save_pretrained(
                                os.path.join(cfg.output_dir, f"global_step{global_step + 1}", "unet_ema"))
                        else:
                            unet.module.save_pretrained(
                                os.path.join(cfg.output_dir, f"global_step{global_step + 1}", "unet"))
                    else:
                        unet_lora_state_dict = convert_state_dict_to_diffusers(
                            get_peft_model_state_dict(unet.module.to(torch.float32)))
                        StableDiffusionPipeline.save_lora_weights(
                            save_directory=os.path.join(cfg.output_dir, f"global_step{global_step + 1}"),
                            unet_lora_layers=unet_lora_state_dict,
                            safe_serialization=True
                        )
                        if weight_dtype == torch.float16:
                            unet.module.to(torch.float16)
                    logging.info(f'æƒé‡è½¬æ¢å®Œæˆ____å½“å‰ä¿å­˜çš„æ˜¯ï¼š{global_step + 1}__loss: {loss.item():.6f}')
                global_step += 1
        if global_step >= cfg.max_train_steps:
            break
        if is_rank_0() and cfg.validation_prompts is not None and cfg.validation_epochs > 0 and epoch % cfg.validation_epochs == 0:
            if not cfg.use_lora.action and cfg.use_ema:
                ema_unet.store(unet.module.parameters())
                ema_unet.copy_to(unet.module.parameters())
            log_validation(unet.module, cfg, device, weight_dtype)
            if not cfg.use_lora.action and cfg.use_ema:
                ema_unet.restore(unet.module.parameters())


if __name__ == '__main__':
    main()
